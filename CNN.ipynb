{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iT-TGFA-PVS_"
      },
      "outputs": [],
      "source": [
        "import tensorflow\n",
        "import numpy\n",
        "from keras.preprocessing.image import ImageDataGenerator as IDG\n",
        "from keras.preprocessing import image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JNkW7CDeQnoS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 2252 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "training_data_generator = IDG(rescale = 1./255,\n",
        "                                   shear_range = 0.1,\n",
        "                                   zoom_range = 0.1,\n",
        "                                   horizontal_flip = True)\n",
        "training_data = training_data_generator.flow_from_directory('male_female_dataset/training_data',\n",
        "                                                 target_size = (128, 128),\n",
        "                                                 batch_size = 32,\n",
        "                                                 class_mode = 'binary')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pfMYxKzs3Xrp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 3015 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "testing_data_generator = IDG(rescale = 1./255)\n",
        "testing_data = testing_data_generator.flow_from_directory('male_female_dataset/testing_data',\n",
        "                                            target_size = (128, 128),\n",
        "                                            batch_size = 32,\n",
        "                                            class_mode = 'binary')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RysOAPk23ZW7"
      },
      "outputs": [],
      "source": [
        "neural_network = tensorflow.keras.models.Sequential()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "EV4u1Buk3c0f"
      },
      "outputs": [],
      "source": [
        "neural_network.add(tensorflow.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[128, 128, 3]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "v8jckSjl3fGq"
      },
      "outputs": [],
      "source": [
        "neural_network.add(tensorflow.keras.layers.MaxPool2D(pool_size=2, strides=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "73GBpPO93hFF"
      },
      "outputs": [],
      "source": [
        "neural_network.add(tensorflow.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
        "neural_network.add(tensorflow.keras.layers.MaxPool2D(pool_size=2, strides=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "bb9ywsPe3ihj"
      },
      "outputs": [],
      "source": [
        "neural_network.add(tensorflow.keras.layers.Flatten())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9u3xV9Un3kZF"
      },
      "outputs": [],
      "source": [
        "neural_network.add(tensorflow.keras.layers.Dense(units=128, activation='relu'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "YYvKOjwx3l7Q"
      },
      "outputs": [],
      "source": [
        "neural_network.add(tensorflow.keras.layers.Dense(units=1, activation='sigmoid'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "DUAOpB-33n9p"
      },
      "outputs": [],
      "source": [
        "neural_network.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "XyxhBDdH3pqd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "71/71 [==============================] - ETA: 0s - loss: 0.6522 - accuracy: 0.6417"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Ahmet Kucuk\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\PIL\\Image.py:945: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "71/71 [==============================] - 169s 2s/step - loss: 0.6522 - accuracy: 0.6417 - val_loss: 0.4842 - val_accuracy: 0.7728\n",
            "Epoch 2/30\n",
            "71/71 [==============================] - 88s 1s/step - loss: 0.4462 - accuracy: 0.7869 - val_loss: 0.4974 - val_accuracy: 0.7632\n",
            "Epoch 3/30\n",
            "71/71 [==============================] - 88s 1s/step - loss: 0.3927 - accuracy: 0.8153 - val_loss: 0.5396 - val_accuracy: 0.7784\n",
            "Epoch 4/30\n",
            "71/71 [==============================] - 88s 1s/step - loss: 0.3269 - accuracy: 0.8646 - val_loss: 0.4183 - val_accuracy: 0.8328\n",
            "Epoch 5/30\n",
            "71/71 [==============================] - 86s 1s/step - loss: 0.2603 - accuracy: 0.8939 - val_loss: 0.4181 - val_accuracy: 0.8478\n",
            "Epoch 6/30\n",
            "71/71 [==============================] - 84s 1s/step - loss: 0.2105 - accuracy: 0.9121 - val_loss: 0.4338 - val_accuracy: 0.8498\n",
            "Epoch 7/30\n",
            "71/71 [==============================] - 84s 1s/step - loss: 0.1895 - accuracy: 0.9267 - val_loss: 0.3785 - val_accuracy: 0.8726\n",
            "Epoch 8/30\n",
            "71/71 [==============================] - 84s 1s/step - loss: 0.1600 - accuracy: 0.9405 - val_loss: 0.4064 - val_accuracy: 0.8896\n",
            "Epoch 9/30\n",
            "71/71 [==============================] - 86s 1s/step - loss: 0.1487 - accuracy: 0.9440 - val_loss: 0.3763 - val_accuracy: 0.8972\n",
            "Epoch 10/30\n",
            "71/71 [==============================] - 84s 1s/step - loss: 0.1125 - accuracy: 0.9596 - val_loss: 0.4707 - val_accuracy: 0.8793\n",
            "Epoch 11/30\n",
            "71/71 [==============================] - 83s 1s/step - loss: 0.1024 - accuracy: 0.9578 - val_loss: 0.5860 - val_accuracy: 0.8769\n",
            "Epoch 12/30\n",
            "71/71 [==============================] - 83s 1s/step - loss: 0.0880 - accuracy: 0.9685 - val_loss: 0.5571 - val_accuracy: 0.8836\n",
            "Epoch 13/30\n",
            "71/71 [==============================] - 83s 1s/step - loss: 0.0768 - accuracy: 0.9707 - val_loss: 0.6290 - val_accuracy: 0.8852\n",
            "Epoch 14/30\n",
            "71/71 [==============================] - 83s 1s/step - loss: 0.0698 - accuracy: 0.9742 - val_loss: 0.3968 - val_accuracy: 0.9121\n",
            "Epoch 15/30\n",
            "71/71 [==============================] - 83s 1s/step - loss: 0.0673 - accuracy: 0.9765 - val_loss: 0.4973 - val_accuracy: 0.9128\n",
            "Epoch 16/30\n",
            "71/71 [==============================] - 84s 1s/step - loss: 0.0497 - accuracy: 0.9831 - val_loss: 0.4740 - val_accuracy: 0.9075\n",
            "Epoch 17/30\n",
            "71/71 [==============================] - 84s 1s/step - loss: 0.0429 - accuracy: 0.9845 - val_loss: 0.4673 - val_accuracy: 0.9161\n",
            "Epoch 18/30\n",
            "71/71 [==============================] - 84s 1s/step - loss: 0.0528 - accuracy: 0.9787 - val_loss: 0.5555 - val_accuracy: 0.9114\n",
            "Epoch 19/30\n",
            "71/71 [==============================] - 84s 1s/step - loss: 0.0543 - accuracy: 0.9831 - val_loss: 0.5197 - val_accuracy: 0.9128\n",
            "Epoch 20/30\n",
            "71/71 [==============================] - 84s 1s/step - loss: 0.0473 - accuracy: 0.9836 - val_loss: 0.6766 - val_accuracy: 0.8982\n",
            "Epoch 21/30\n",
            "71/71 [==============================] - 84s 1s/step - loss: 0.0299 - accuracy: 0.9902 - val_loss: 0.5502 - val_accuracy: 0.9081\n",
            "Epoch 22/30\n",
            "71/71 [==============================] - 84s 1s/step - loss: 0.0275 - accuracy: 0.9929 - val_loss: 0.6738 - val_accuracy: 0.9005\n",
            "Epoch 23/30\n",
            "71/71 [==============================] - 84s 1s/step - loss: 0.0178 - accuracy: 0.9933 - val_loss: 0.7074 - val_accuracy: 0.8982\n",
            "Epoch 24/30\n",
            "71/71 [==============================] - 84s 1s/step - loss: 0.0256 - accuracy: 0.9925 - val_loss: 0.7001 - val_accuracy: 0.8995\n",
            "Epoch 25/30\n",
            "71/71 [==============================] - 84s 1s/step - loss: 0.0280 - accuracy: 0.9893 - val_loss: 0.5894 - val_accuracy: 0.9250\n",
            "Epoch 26/30\n",
            "71/71 [==============================] - 84s 1s/step - loss: 0.0351 - accuracy: 0.9920 - val_loss: 0.6303 - val_accuracy: 0.9065\n",
            "Epoch 27/30\n",
            "71/71 [==============================] - 84s 1s/step - loss: 0.0266 - accuracy: 0.9898 - val_loss: 0.6123 - val_accuracy: 0.9028\n",
            "Epoch 28/30\n",
            "71/71 [==============================] - 84s 1s/step - loss: 0.0295 - accuracy: 0.9893 - val_loss: 0.7207 - val_accuracy: 0.9025\n",
            "Epoch 29/30\n",
            "71/71 [==============================] - 84s 1s/step - loss: 0.0203 - accuracy: 0.9938 - val_loss: 0.5751 - val_accuracy: 0.9124\n",
            "Epoch 30/30\n",
            "71/71 [==============================] - 85s 1s/step - loss: 0.0114 - accuracy: 0.9969 - val_loss: 0.6861 - val_accuracy: 0.9148\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x227d9891a30>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "neural_network.fit(x = training_data, validation_data = testing_data, epochs = 30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "HJBtr4pv3r0L"
      },
      "outputs": [],
      "source": [
        "single_prediction = image.load_img('male_female_dataset/single_prediction/man_or_woman9.jpg', target_size = (128, 128))\n",
        "single_prediction = image.img_to_array(single_prediction)\n",
        "single_prediction = numpy.expand_dims(single_prediction, axis = 0)\n",
        "probability = neural_network.predict(single_prediction/255.0)\n",
        "training_data.class_indices\n",
        "if probability[0][0] > 0.5:\n",
        "  gender = 'woman'\n",
        "else:\n",
        "  gender = 'man'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "3O0HK53c3tjp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "woman\n"
          ]
        }
      ],
      "source": [
        "print(gender)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "CNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
